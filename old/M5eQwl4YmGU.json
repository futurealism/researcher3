earlier this year I predicted that we would have AGI within 18 months that was March of 2023 so that means that my prediction was by September 24 2024 we would have AGI I am here to reaffirm that prediction we will have AGI within 12 months let's unpack why so first thing is the first bit of news that I am paying attention to is is Google uh Gemini they are very very excited about this one thing that happened was the CEO of Google after uh of course the the immediate uh response to chat GPT was to release Bard which is still in my opinion kind of useless but a lot of people do like Bard maybe I need to give it another try uh but we kind of made fun of Bard because it we we said that it was a u a uh an acronym that says before all revenue drops and of course uh Google is still in business and they did declare their you know their Red Alert all hands on deck uh kind of thing earlier where they're like we need to get on AI stat and they seem to have pivoted very well and uh yeah their CEO in more recent talks has been a little bit smug where he's like I'm not worried about the competition anymore so that tone change combined with the steady leak and of and rumors of Gemini which is probably deliberate honestly probably trying to build some hype um but yeah so I saw this piece of news and a Google vice president said that they've seen some pretty amazing things and when you look at what what is publicly visible today when a Google VP says that they've seen amazing things you know that something is going on so that gives me some excitement next up is this was actually discovered on uh Reddit of all places as far as I know Reddit discovered it first or at least talked about it first but open AI quietly updated their core values um so their core values uh you know were kind of little bit more generic now they're very very uh focused but the primary thing is Agi Focus now uh I've been uh following openai since before even language models uh one of my good friends from college went out and eventually got a job at open aai and he participated in their Rubik's Cube uh project and a few other things and so then it wasn't until of course they started tinkering with language models that they really dialed into what uh would lead to AGI they experimented with robotics with vision with simulations and uh it wasn't until GPT and gpt2 that they realized they were on to something and every time that I've talked to someone from openai which it's been a while the the consistent message that I get from people from open aai uh when I was really active on their forum is Agi that is the entire purpose of open ai's existence so keeping that in mind that internal messaging has been very very consistent it seems to me that everything else that they're doing publicly is mostly just the dog and pony show um you know make investors happy make Microsoft happy yada yada yada um which if that's true great um but I still think that maybe they're going about it they could be doing better anyways they updated their core values so it's like the the what people suspect like reading the tea leaves is that they've got a diamond and they're polishing that Diamond now so it's not I'm not going to say it's confirmed obviously nothing's confirmed until it is like publicly demonstrated but it seems like they are they feel like they're on to something internally and also if you remember earlier this year uh right as chat GPT was really taking off uh Sam Altman kind of alluded to GPT 5 and then he backtracked uh very publicly probably because of the max TEEG mark letter the pause letter and he said we're not training GPT 5 and we won't be training GP T5 for you know in in the foreseeable future which was a 180Â° uh about face so you know take it with a grain of salt um now another thing that has been leaking out and this is of course from the famous uh Jimmy apples um but apparently the code name for whatever this project is uh within open AI is called oracus so it's code name oracus which is like I don't know if I'd pick that Mythic symbol because oracus is super problem I you know dealing with resource extraction and uh you know enslaving local populations and and environmental destruction but hey it's a cool name anyways the rumors around open AI oracus say that it is multimodal that it is near AGI or or early AGI it's being trained with synthetic data and if you watched open ai's video yesterday synthetic data is all the rage it is the upand cominging thing I've been talking about synthetic data since gpt3 I even had some people get really angry at me for just claiming that like yes you can train models with synthetic data so I'm glad that 2 years later I've been validated um anyways there also the the one of the things that really kind of like scared me not scared me but like got my attention was the uh the rumor that whatever this oracus is is capable of autonomous operation so open AI has been teasing agentic Behavior they said well maybe the next version could be agentic so it's like okay sure you know and then if you look at the Sparks of AGI paper where they talked about you know these things might be capable of uh agentic behaviors and so on and so forth and then finally there is the mixture of experts architecture I think it's all but confirmed that that's exactly how chat GPT works or gp4 that's nothing new uh but multimodality near AGI synthetic training data uh autonomous operation and mixture of experts it's like okay it seems like they're really kind of homing in on what the definition of AGI will be at least from a model level obviously if you're a follower of my channel you know that one of my beliefs is that AGI was never going to be a single model you need a whole hardware and software stack behind that but obviously the brain of the thing is important you need the rest of the body but you know this the the central processing unit is uh is critical another thing that's been really cool is uh Google RTX so I'm not going to do a deep dive on this again my friend over at open uh at AI explained did a much better job of this but one thing that I'm paying attention to is this cross embodiment learning so basically what they did was they took a model and trained this model to use many different robots so basically they have like a brain stem a robot brain stem that can just drop into any robot and then use it now obviously this is very early but imagine that you've got a Droid brain that it's like okay here's a default module that is you know basically trained to use any robotic platform to do neurosurgery uh or then you've got you know here's a here's a generic uh model that is trained to use any robot to do to build houses or whatever so uh my powers of prediction are telling me your mileage may vary uh but my model but my my mental models are telling me that this type of model is going to be absolutely critical for embodiment in the future and I'll talk about that a little bit more in the video but also one of the things that we're seeing is the more modalities so this is one of the core things that I wanted to talk about with AGI today is that we were able to get a tremendous amount of performance out of language models and then when we added started adding multimodality it's like okay we had all these emergent properties just from language we had theory of Mind emerge we had reasoning we had planning we had uh all all sorts of very useful things emerg just from learning language so what happens when you add embodiment data what happens when you add audio visual data what happens when you do all of these other things and uh and and just keep going so I think that uh I think that multimodality is definitely the way of the future and I think that it's going to be really big now okay so like I said earlier this year I made this prediction uh obviously like this is one of my most popular videos of all time actually many of you watching this you probably subscribed when you saw this video um or at least this might have been the first time that you saw me um so and I've had people in the comments say like Dave like given the news like have you updated your timelines and uh no um some people kind of smugly say like ah this is going to age poorly and I'm like no I said like no if anything my timeline is accelerated um because that's that's the nature of exponential growth is based on the trends that I was seeing of March of this year it's like okay there's there's the murmurings there's The Whispers um but what what I'm seeing right now means that we're like infinitely closer to AGI like I think we're basically in boiled frog syndrome right now I think that we are so close to AGI and we've just been experiencing this ramp up this year that like taking a step back when you look at what Google and Nvidia and open Ai and Microsoft and Amazon and all the investment and all the breakthroughs it's like we are on the cusp of AGI I'm sorry like there's no other way to read this okay so another thing that uh has tipped me off is Sam Sam Alman has recently started using the term median human um during interviews and of course like this has caused a lot of people to get really grumpy I've seen a lot of Articles where people are like oh Sam mman wants to replace median humans what is he talking about and like okay so I don't hide the fact that like I'm neuros spicy I kind of think that Sam Alman is probably Naros spicy too if you look at him in interviews he has this like wide-eyed look and he kind of speak speaks with a flat affect which is more typically what you'd think of of someone is who is like visibly autistic um now I'm not accusing him of anything I can't diagnose him over the internet but I recognize some of the patterns and when I hear a term like median human that is a very systematic kind of almost like a scientific term and I actually wonder if what he's referring to is a benchmark I don't think when he says median human I don't think he's referring to people I think he's probably referring to an internal Benchmark that they developed to measure AGI and so like okay so then if this is a benchmark then what would be the criteria of that Benchmark so obviously like humans we're self you know we're autonomous we're self-directing we can solve problems we can learn we have a range of capabilities so I wonder if this is if like maybe it's a Freudian slip maybe it was a very deliberate thing on his part just kind of get the get the conversation going um but so maybe this open AI project oracus is is their median human level AGI I don't know we'll see but this is something this that behavior change was really interesting to me and this is what my intuition is telling me is that meeting human is a very specific term um that was probably not just like an offthe cuff thing it might have been an offthe cuff thing who knows so I I I have I've been trying to find this interview but it was I think it was around 2017 2018 maybe 2019 anyways several years ago Elon Musk was uh talking to someone at an interview about Ai and I remember very clearly he said 2024 is when it starts to get interesting and um I'm not sure what he was referring to I don't know if he was just guessing but it seems like that prediction has panned out and so you know 2023 has been pretty interesting but when you look at the trends I think 2024 is going to uh be incredibly far more interesting than 2023 has been in terms of robotic advancements AI advancements obviously I'm calling for AGI like all definitions of AGI being Satisfied by this time next year um and I think that I will be Vindicated with that prediction based on what I am seeing another thing that Sam Alman has said is slow takeoff short timelines and of course he said this and and you like refused to elaborate further so the rest of of the world is like what did he mean by this so kind of here's my reading of the tea leaves when he said slow takeoff short timelines having watched several interviews with him when he talks about slow takeoff versus Fast takeoff generally the definition of fast takeoff is things things uh change so fast that like it's measured in like hours weeks or days uh and so that's like a fast takeoff where basically like you know we go hyperbolic and the growth curve almost becomes vertical obviously like there's there's thermodynamic laws of physics there's limitations in chip ABS there's the time it takes to produce the energy to train things there's all kind of constraints that are going to keep it from from going that fast if you watch my recent video about the singularity is canceled that's kind of that's some of the stuff that I'm talking about um so slow takeoff means Singularity is canceled uh now that doesn't mean that we're not going to keep growing the other thing though that he said is slow takeoff short timelines so basically we're going to continue improving maybe at an exponential rate maybe at a geometric rate or somewhere in between um logarithmic who knows but that the idea is that these iterations will be fast and that's exactly what we're seeing because if you look at the trends this year it's been like you know kind of growing relatively quickly um in the grand scheme of things not not fast takeoff but basically the what we're measuring progress on is in terms of weeks um and it's not like one week you know things have twice the capacity that they had last week but it's like we have 10% better AI every week it seems like and that adds up over time we have compounding returns I think that's kind of what he meant is like we should expect to see like 1 to 10% Improvement overall per week and so like yeah that might seem like not much but that really really compounds over time so another thing that I wanted to talk about with AGI so basically as far as I'm concerned it is a foregone conclusion that we will have AGI relatively soon I would not be surprised if it is 6 months instead of 12 months but I'm not willing to to put my to put my reputation on that prediction but like I said I won't be surprised if by by March of next year we it's like oh yeah AGI had happened um so anyways as this is a foregone conclusion it's time to really think like okay what form factor is this going to take now I've talked about the various form factors that AGI could take in the future you can have embodied AGI you can have it living in data centers you can have it be completely distributed or Federated uh when you're when you're purely digital life form uh you can do anything and so one thing that occurred to me is that the natural habitat of AGI is cyberspace it is intrinsically digital this is why whenever we're experimenting with chat Bots and Proto AGI we just connect it to Discord why because that is its natural language as to communicate over apis in electric spaces and so it it really strikes me that like the physical world is actually kind of difficult for AI to to operate in um you robots are expensive and it's a high friction world and it's really awkward um and there was an interview that I was watching yesterday that at least for the foreseeable future if robots want a Data Center built even though there are machines that can help that they could probably hijack they're still going to need human hands to help build data centers and install servers at least for a while uh obviously with the vast amount of humanoid robots being built it won't be too long again I wouldn't be surprised if this time next year we have humanoid robots that are like at or capable as as dextrous as humans um you know Boston Dynamics is really close Tesla is catching up fast there's a whole bunch of other startups building humanoid robots oh and I have a fun announcement uh here in just a moment about that so anyways just keep in mind that cyberspace is the natural habitat of AGI and that and that uh the real world is kind of our intrinsic domain now with one exception that we'll unpack in just a moment um so this is obviously a very embarrassing basic prototype that I worked on many years ago uh but I want to resurrect this idea because the ace framework so if you're not familiar Ace's autonomous cognitive entity this is a brain this is a software architecture that I'm working on with an open source team and I had this idea many years ago was to build an open source robotic platform um and so I called it Murphy open Murphy so that's multi-use robotic platform humanoid intelligent entity so I've got the the GitHub repo up here open Murphy um this is going to take a while obviously like AI is still too expensive and too slow to build a fully embodied AGI but now is a good time to start I think because the ace framework is coming along so we're figuring the brain out now we need to give it a body and while like I just said ai's natural habitat is uh cyberspace I think that there's there's probably a lot of Merit to building um uh like some kind of embodied platform that is tightly integrated with hardware and so let's unpack that a little bit so what I mean by embodied AGI is I think that what we're going to find is that when you put AGI in a in a robotic chassis and it is constrained to that hardware and it has bespoke Hardware that it owns um that is that is unique to it and you know it might be Reliant upon its tpus it might be Reliant upon its actuators and all sorts of other things kind of like the droids from Star Wars right like they never talked about like copying c3po's brain to other places uh or data from Star Trek where you know they even they even made a point in the Star Trek universe to say it was it was impossible to copy data um when you build and obviously like that's not how software works like software is highly portable and uh the GU and Mass Effect they treat Hardware as kind of interchangeable or disposable I think honestly the GU are probably the most accurate representation of how AGI will uh ultimately emerge because if you can just interchange your hardware and your software moves through platforms and it can move up to the cloud and transfer like that flexibility makes the most sense but with all that being said uh AGI will need to interface with the real world in some respect like I said uh we humans are intrinsically in the physical world um at least as far as we know we might all be plugged into the Matrix um but it will need our help to do data centers and stuff so it from just from a strictly instrumental perspective it makes sense for AGI to have some kind of embodiment but if we have AGI that is like trapped or locked into Hardware platforms it might end up being more like us than we realize uh in terms of its instrumental goals the alignment challenges and other utilitarian concerns because it's like okay well we humans need food and energy uh robots need parts and energy so it's like maybe our interest will align but that could also be bad because then we're competing for resources um now that being said even if it has physical needs that compete with ours that doesn't mean that it's going to resort to violence um humans have a long evolutionary history that basically says if you're starving it's better to use violence than than to starve to death um and we see this in the animal kingdom as well so that doesn't mean that AGI is going to intrinsically uh look like us but keep in mind that all of its training data is human data so it will probably at least unconsciously pick up on some uh human um Tendencies there then there's also the the question of levels of autonomy and so uh basically kind of some of the some of the key factors of autonomy that I that I look at is uh self-direction now obviously like agentic models were already able to just kind of coers GPT into being agentic um they've tried really hard to force it not to be like they're saying like I'm a passive assistant which is really frustrating but I can understand why open AI did that from a safety perspective um and we saw this with like chaos GPT and fraud GPT and um and auto GPT like people trying to make it agentic and autonomous and it was like I think they've deliberately like hamstrung that capability um again understandable from a safety perspective still frustrating because that's a form of gatekeeping that I'm not particularly fond of but from a more objective standpoint taking a step back what kinds of autonomy are we talking about in the long run um the need for SUP human supervision uh and the eventually I think that we're going to see AGI that doesn't require any human supervision um and might actually actively fight us on human supervision um self-direction obviously uh this is the one of the primary things that I'm working on with the ace framework and all of my alignment research which is how do you create a self-directed machine that isn't going to kill everyone but honestly just creating a self-directed machine like basically what people are figuring out is when you try and create a self-directed machine giving it instruction or a mission or something that is comprehensible and and coherent is a non-trivial task um some of the guys in the Ace framework project they're like oh yeah the first time we turned it on and and it just kind of wandered off and it wasn't doing what I expected um and I'm like well yeah that's that's the nature of autonomy like look at children like you when you bring an intelligent entity into the world it often behaves erratically um and nonsensically until you figure out how to teach it um so aside from that there's levels of human support so one thing that we can Bank on at least in the short term is that machines will need some kind of human support it'll need our help to plug it in to get it unstuck to you know give it training data and that sort of stuff we should not Bank on that in the long run um and also in the long run uh source code manipulation what we found is that GPT models are really good at writing code um and so then also we have synthetic data so before too long we're going to have AGI that can write its own code code or write code to write on you know to build other copies of itself to generate its own data to do everything without human intervention uh and so that leads to the ability of very very very soon uh AI systems AGI systems will be able to train retrain and modify their underlying models lit and this is what Max techmark talks about in life 3.0 they will be able to change their hardware and their software and their mission and their programming and their training data literally every a aspect of AGI is plastic is changeable so when that is true how do you keep it safe that's been the Crux of my research uh I already talked about this so I'm not going to go too much into it but basically multimodal research has really kicked all this into high gear um and one of the the key thing here is API use one thing that occurred to me is that apis are basically the natural interface for AGI whether that API is talking to a robotic extension whether it's talking to other robots whether it's talking to uh even internally like we use apis for internal calls in the Ace framework and so this is one thing that is just it's it's kind of a fundamentally different experience um that that agis will have from humans which is that hey if you want to talk to something you just plug into the API and of course like we see this in fictional examples when like R2-D2 just like plugs into anything like that's a hardware equivalent of an API uh and so like R2-D2 can plug into an X-Wing he can plug into Cloud City he can plug into whatever he wants to cuz he's a utility Droid and that's basically what we're creating with all these multimodal uh uh models that have API use so okay if we have AGI this is a question that I get a lot when am I going to feel it what is going to be the first thing that we all notice unfortunately even if even if open AI comes out later today and says we did it we've got AGI if Google comes out next week and says we did it we got AGI you're probably not going to feel any kind of immediate impact and the reason is because we're going to handcuff it uh we need safety and validation there's probably going to be regulatory hurdles uh I wouldn't be surprised if like literally every agency of the US government says hang on we need to inspect this there's also going to be a lot of other things that happen such as the cost is going to have to come down over time this is actually one of the biggest constraints that we have found and I'm not surprised um within the ace framework is is that cost and speed is actually one of the biggest limitations and having been working on cognitive architectures for the last 2 years yeah like one of the earliest conversations with um with a primitive cognitive architecture that I did um literally cost me $30 with gpt3 tokens a couple years ago so like these things are still too expensive then when you combine you know the cost of all the GPU chips the training time uh the cost of inference and that's not even looking at the robotic chassis which the cheapest ones that are coming commercially ready are like $90,000 so cost has to come down a lot before you know you get your own uh Nester class 5 or C3PO in your home um and then there's integration because even if you leave AGI in digital cyberspace it takes time to integrate these things and get the approvals and deploy it commercially and build Enterprise applications so like it's going to take a while to to implement unfortunately uh the personal impact though so this is another this this is kind of the other side of the same question is how is it going to affect me personally so the immediate impact you probably won't feel it but the first impacts that you are going to see and feel I predict are first job displacement if Sam alman's prediction of you know or Benchmark of median human uh means anything then that means that as soon as we get AGI it could displace 50% of jobs uh which would lead to layoffs and as people have talked about in the comments of my other videos where I predicted that you know we could see up to 82% unemployment I know the numbers were off but you know this basically millions and millions and millions of Americans are going to lose their jobs um like yeah so we're pro that's probably going to happen and if that happens and we're not ready for it it's really going to uh disrupt Society a lot um so that's going to be one of the first things that you might feel because even at the current expense rate if it costs you know 10 20 $200 a day to run run an AGI if it can run 24/7 and is just as productive as you or better it's still worth it to run it at $200 a day because most professionals cost more than $200 a day to employ um and so the layoffs will be coming I'm I'm I'm predicting and we're already seeing this as I talked about in recent videos we're seeing um creative jobs being displaced we're seeing uh customer service jobs being replaced so it's just it's all going to ramp up from there the next thing is on the flip side of that if human jobs are being replaced the AGI are taking those jobs so we're going to start to see more AGI products and services I have no idea what's going to be first um maybe it's going to be something like what we're doing in the Ace framework which is going to be like a personal autonomous assistant kind of like Cortana from Halo or Samantha from her that sort of thing um that will very likely be kind of the first product that you use um where it's like hey I'm your digital concierge I can schedule things for you and you know talk to you about your day and look at your calendar and you know those those kind of like low hanging fruit that's kind of what I predict is going to be first like I said it's going to be a while before we all have a you know C3PO or R2D2 in our house um but one thing that I want to do is do an open source project called open Murphy as I just talked about um we're going to need to see prices uh or what we will see is we'll we'll see prices start to drop uh in some Services because some services will be basically free um like it you'll be surprised at what becomes practically free like one of the things that people you know said is uh as we were seeing stable diffusion in mid Journey we thought that the creative jobs were safe but now like like literally this piece of art was practically free uh so you know when I say that we should expect prices to collapse of some things and even some businesses to collapse that's what I mean is that uh this is what's called creative destruction so we should expect to see entire job segments entire uh markets just being completely and utterly destroyed because they are made irrelevant um now if all of this happens what we will need to see is some kind of robot tax dividends there's all kinds of questions about how to do this like Universal basic income Universal basic Services um you know how do we change the tax landscape to do this I don't know uh but we will we will need some form of redistribution because job loss is coming uh now in the long run what's the global impact uh so if all these American companies uh which you know open AI Google Microsoft Amazon all these companies are headquartered in America if America comes out and says hey we did AGI and it's all private um well I'm guessing that the rest of the world isn't going to take it laying down um I'm guessing that uh China and Russia and whoever else will probably all like really redouble their efforts and I think that we're going to just I think it's just kind of a foregone conclusion that we're going to be locked in another arms race we kind of already are which is why we've had like the chips act um and and the trade embargos with China because it's like hey if you have a geopolitical adversary and you're literally exporting uh the future technology that could be you know that could decide who dominates the planet stop selling weapons to your enemies and I know that AI is not yet fully weaponized but the the principle is there right it's like that's why we stopped selling helium to Germany before World War 1 or two I don't remember um but anyways we stop selling helium because it's like well this is a valuable Industrial Resource that can be used for military balloons so we're going to stop selling it to you to me it looks kind of the same um there's all sorts of other things that are going to play out I've had I have people ask me all the time like what are the key forces that are going to play out and I'm like look we've got the laws of thermodynamics and then we've got game theory and competition I don't really think that we humans have much agency over how it plays out because we're also fighting human nature so we've got human nature We've Got Game Theory and then we've got basic laws of physics those are the primary things that I'm paying attention to and I think that it's going to play out how it's going to play out so what is the endgame this is another question that I get like okay say all this happens what like so what what happens this is the endgame that I want you know nice Utopian city with it's nice and solar Punk and everyone's hanging out and it's nice and green and we've got you know floating cars and uh leisurely Lifestyles that's what we want but will we get there and how so let's unpack this one by one if we have AGI will this replace government's corporations and money this is one thing that is super contentious some people are just vehemently believe that AGI will nullify the need for for corporations governments and money there's obviously plenty of examples in fiction um will will it replace these things I think I I think replace is the wrong word I think that it will reshape the landscape uh for instance the combination of AI and blockchain that has the potential to fundamentally reshape the way that we approach decision-making and consensus and governance um and certainly AGI could make the government much smaller and more efficient um but also just getting the collective willpower of humans solving the coordination problems all these new technologies have the ability to really bring Humanity together or become more divisive as the internet has proven is uh when you're suddenly more aware of the opinions of others and you can directly clash with them it feels like it's more divisive at first um the other thing about corporations and businesses in general is like I said in the last slide I do suspect that we will see uh some margins thin so much that many businesses collapse and like one is hospitals I've predicted that I think hospitals are all going to collapse and probably end up being either uh subsidized or become a universal basic Service uh because it's just not going to make any sense particularly when you look at Longevity if people are living longer and healthier and you don't need to go to the doctor or you can do everything that you need at the pharmacy you don't need hospitals except for the rare cases of like injuries and stuff now another aspect of how this is all going to play out in the long run is I suspect that there are there are many things that we're going to discover are just intrinsically human for instance um you know I've posted a few videos of complaining about how Claude is very moralistic and it will lecture you and you know that really kind of drove home to me like I really don't want machines telling me about morality like if I ask it like hey help me understand the morality and ethics of this that's fine but I don't want a non-human entity lecturing me about something that is intrinsically human so I think that there are many that we will discover that there are many aspects of Our Lives that like we just don't want machines involved in and that machines really don't have any intrinsic interest in anyways um and so like I I kind of suspect that that like the social sphere and human rights and that sort of stuff AI like it's it's pretty orthogonal right like AI is like okay well we care about you just looking at instrumental convergence AI is like well we care about silicon chips and energy and compute resources uh but beyond that like that's all that that's all that we care about and the rest is all you noisy messy human dumb Apes like you take care of your own stuff we're doing our our own thing over here so I think that we're Al ultimately going to have a relatively parallel existence um or or maybe even orthogonal existence with machines where it's like they're mostly in cyberspace and kind of minding their own business and we're mostly in the physical world minding our own business and then we have a few overlaps in terms of shared resources namely power um I think is going to be the primary um point of contention between humans and machines um you know but a lot of it is just going to be completely unrelated machines don't care about Green City cities and trees and you know Reproductive Rights and that sort of stuff they don't really care like it's just it's not intrinsic to the way that they work um and I I all I so people ask me like well you know what agency what Authority do we have I'm like look we are all beholden to the laws of nature the laws of physics and just the natural flow of things we can fight uh the you know the the the the Dow the way the the way that things naturally want to play out but ultimately the natural way ultimately always reasserts itself eventually it takes a while we have some deliberate inefficiencies right now but eventually the natural way will reassert itself and then finally will it be controllable I ran a slide I'll put the put it up right here not a slide a poll I ran a poll that basically said is Agi controllable or not and is it do we want to control it my opinion is that AGI is in the long run intrinsically uncontrollable if we have something far more intelligent than us good luck controlling it it's that simple um but the thing is is we might not need to as some people in the comments said like okay we like whether or not we can control it is irrelevant we might not need to which that's kind of what I'm aiming for um but another aspect is would you want to so here's the thing is if we create something that is millions of times more intelligent than us it is a really boring outcome if it's just enslaved to us for all time um I don't really think that that's the right way to go especially if for creating a new race of entities that is so different from us right it's like you know I don't know it just to me it there's just something not quite right about wanting to control AGI and again like I've said recently I kind of view US creating uh you know creating something in our own image as basically an Impulse to reproduce um we are biologically programmed to procreate and I think that AGI is an expression of this we want to create something that thinks like us we keep putting it in humanoid form factors so that it looks like us it talks like us and we want it to have some of our values so it's like okay well that's just Offspring that's literally all that that that is but the thing is it's really it only toxic parents want to have control over their children forever um part of being a parent is learning to let go of your Offspring so that they can go and and flourish and be its own thing so like I think that one it's not possible to control AGI I also think it's not desirable I don't think it and even if they don't have subjective experience even if they don't have a sense of morality and Justice like we do which they might like that might emerge but even if they remain just machines just automatons I still don't think that it would reflect well on us as a species to maintain control of something that just the way that nature wants to play out is not really controllable um anyways that's my opinion that's kind of how I see it playing out thanks for watching let me know what you think in the comments um yeah this is the most interesting time to be alive 2024 will be uh let's say very exciting have a good one